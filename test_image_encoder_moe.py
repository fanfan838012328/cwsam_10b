"""
ä¸“é—¨æµ‹è¯•å›¾åƒç¼–ç å™¨ä¸­çš„MoEå®ç°
"""

import torch
import yaml
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/mnt/fanfq/project/code/cwsam_10b')

import models


def test_image_encoder_moe():
    """ä¸“é—¨æµ‹è¯•å›¾åƒç¼–ç å™¨çš„MoEå®ç°"""
    print("æµ‹è¯•å›¾åƒç¼–ç å™¨MoEå®ç°...")
    
    # 1. åŠ è½½é…ç½®
    config_path = '/mnt/fanfq/project/code/cwsam_10b/configs/XinTong/XinTong_sam_vit_h_moe_3b.yaml'
    with open(config_path, 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    
    print(f"æ¨¡å‹é…ç½®: {config['model']['name']}")
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = models.make(config['model'])
    
    # 3. æ£€æŸ¥å›¾åƒç¼–ç å™¨
    image_encoder = None
    if hasattr(model, 'image_encoder'):
        image_encoder = model.image_encoder
        print(f"å›¾åƒç¼–ç å™¨ç±»å‹: {type(image_encoder)}")
        print(f"å›¾åƒç¼–ç å™¨æ¨¡å—: {type(image_encoder).__module__}")
    else:
        print("æœªæ‰¾åˆ°å›¾åƒç¼–ç å™¨!")
        return False
    
    # 4. æ£€æŸ¥å›¾åƒç¼–ç å™¨çš„blocks
    if hasattr(image_encoder, 'blocks'):
        print(f"\nå›¾åƒç¼–ç å™¨å±‚æ•°: {len(image_encoder.blocks)}")
        
        # æ£€æŸ¥MoEå¼€å§‹å±‚
        moe_start_layer = getattr(image_encoder, 'moe_start_layer_index', 24)
        print(f"MoEå¼€å§‹å±‚: {moe_start_layer}")
        
        # æ£€æŸ¥æ¯ä¸€å±‚çš„MLPç±»å‹
        hierarchical_moe_count = 0
        optimized_moe_count = 0
        standard_mlp_count = 0
        
        for i, block in enumerate(image_encoder.blocks):
            block_type = type(block).__name__
            mlp_type = type(block.mlp).__name__ if hasattr(block, 'mlp') else 'None'
            mlp_module = type(block.mlp).__module__ if hasattr(block, 'mlp') else 'None'
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨åˆ†å±‚MoE
            use_hierarchical_moe = getattr(block, 'use_hierarchical_moe', False)
            
            if i < 5 or i >= moe_start_layer:  # æ˜¾ç¤ºå‰5å±‚å’ŒMoEå±‚
                print(f"  Layer {i}: Block={block_type}, MLP={mlp_type}")
                print(f"           æ¨¡å—={mlp_module}")
                print(f"           use_hierarchical_moe={use_hierarchical_moe}")
                
                # æ£€æŸ¥é«˜æ•ˆMoEç‰¹å¾
                if hasattr(block.mlp, 'load_1_5b_expert_weights'):
                    print(f"           âœ“ åŒ…å« load_1_5b_expert_weights")
                    optimized_moe_count += 1
                elif 'HierarchicalMoE' in mlp_type:
                    print(f"           åŸå§‹åˆ†å±‚MoE")
                    hierarchical_moe_count += 1
                else:
                    print(f"           æ ‡å‡†MLP")
                    standard_mlp_count += 1
        
        print(f"\nç»Ÿè®¡ç»“æœ:")
        print(f"  ä¼˜åŒ–åˆ†å±‚MoEå±‚: {optimized_moe_count}")
        print(f"  åŸå§‹åˆ†å±‚MoEå±‚: {hierarchical_moe_count}")
        print(f"  æ ‡å‡†MLPå±‚: {standard_mlp_count}")
        
        # 5. ç‰¹åˆ«æ£€æŸ¥ç¬¬24å±‚ä»¥åçš„å±‚ï¼ˆåº”è¯¥ä½¿ç”¨åˆ†å±‚MoEï¼‰
        print(f"\næ£€æŸ¥MoEå±‚ (Layer {moe_start_layer} ä»¥å):")
        for i in range(moe_start_layer, len(image_encoder.blocks)):
            block = image_encoder.blocks[i]
            if hasattr(block, 'mlp'):
                mlp_type = type(block.mlp).__name__
                mlp_module = type(block.mlp).__module__
                
                print(f"  Layer {i}: {mlp_type}")
                
                # æ£€æŸ¥å…³é”®æ–¹æ³•
                has_load_weights = hasattr(block.mlp, 'load_1_5b_expert_weights')
                has_freeze_experts = hasattr(block.mlp, '_freeze_original_experts')
                has_expert_choice = hasattr(block.mlp, 'expert_choice_routing')
                
                print(f"           load_1_5b_expert_weights: {'âœ“' if has_load_weights else 'âœ—'}")
                print(f"           _freeze_original_experts: {'âœ“' if has_freeze_experts else 'âœ—'}")
                print(f"           expert_choice_routing: {'âœ“' if has_expert_choice else 'âœ—'}")
                
                if has_load_weights:
                    return True
        
        return optimized_moe_count > 0
    else:
        print("å›¾åƒç¼–ç å™¨æ²¡æœ‰blocks!")
        return False


def test_hierarchical_block_creation():
    """æµ‹è¯•HierarchicalBlockçš„åˆ›å»ºè¿‡ç¨‹"""
    print("\n" + "="*60)
    print("æµ‹è¯•HierarchicalBlockåˆ›å»ºè¿‡ç¨‹")
    print("="*60)
    
    # ç›´æ¥æµ‹è¯•HierarchicalBlockçš„åˆ›å»º
    from models.mmseg.models.sam.image_encoder_moe_layer import HierarchicalBlock
    
    print("åˆ›å»ºHierarchicalBlock...")
    try:
        block = HierarchicalBlock(
            dim=1280,
            num_heads=16,
            mlp_ratio=4.0,
            num_expert_groups=6,
            experts_per_group=16,
            k_groups=2,
            k_experts=4,
            use_hierarchical_moe=True,  # å…³é”®å‚æ•°
        )
        
        print(f"âœ“ HierarchicalBlockåˆ›å»ºæˆåŠŸ")
        print(f"  MLPç±»å‹: {type(block.mlp)}")
        print(f"  MLPæ¨¡å—: {type(block.mlp).__module__}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é«˜æ•ˆMoEçš„æ–¹æ³•
        has_load_weights = hasattr(block.mlp, 'load_1_5b_expert_weights')
        has_freeze_experts = hasattr(block.mlp, '_freeze_original_experts')
        
        print(f"  load_1_5b_expert_weights: {'âœ“' if has_load_weights else 'âœ—'}")
        print(f"  _freeze_original_experts: {'âœ“' if has_freeze_experts else 'âœ—'}")
        
        if has_load_weights:
            print("  âœ“ ä½¿ç”¨äº†OptimizedHierarchicalMoEMLPBlock")
            return True
        else:
            print("  âœ— ä½¿ç”¨äº†åŸå§‹HierarchicalMoEMLPBlock")
            return False
            
    except Exception as e:
        print(f"âœ— HierarchicalBlockåˆ›å»ºå¤±è´¥: {e}")
        return False


def test_import_in_context():
    """åœ¨HierarchicalBlockçš„ä¸Šä¸‹æ–‡ä¸­æµ‹è¯•å¯¼å…¥"""
    print("\næµ‹è¯•åœ¨HierarchicalBlockä¸Šä¸‹æ–‡ä¸­çš„å¯¼å…¥:")
    
    # æ¨¡æ‹ŸHierarchicalBlock.__init__ä¸­çš„å¯¼å…¥é€»è¾‘
    use_hierarchical_moe = True
    
    if use_hierarchical_moe:
        print("  å°è¯•å¯¼å…¥OptimizedHierarchicalMoEMLPBlock...")
        try:
            from models.mmseg.models.sam.efficient_moe import OptimizedHierarchicalMoEMLPBlock
            print("  âœ“ å¯¼å…¥æˆåŠŸ")
            
            # å°è¯•åˆ›å»ºå®ä¾‹
            try:
                mlp = OptimizedHierarchicalMoEMLPBlock(
                    embedding_dim=1280,
                    mlp_dim=5120,
                    num_expert_groups=6,
                    experts_per_group=16,
                    k_groups=2,
                    k_experts=4,
                    expert_capacity_factor=1.5,
                    use_checkpoint=False,
                )
                print("  âœ“ å®ä¾‹åŒ–æˆåŠŸ")
                print("  âœ“ åº”è¯¥ä½¿ç”¨OptimizedHierarchicalMoEMLPBlock")
                return True
            except Exception as e:
                print(f"  âœ— å®ä¾‹åŒ–å¤±è´¥: {e}")
                print("  å°†å›é€€åˆ°HierarchicalMoEMLPBlock")
                return False
                
        except ImportError as e:
            print(f"  âœ— å¯¼å…¥å¤±è´¥: {e}")
            print("  å°†å›é€€åˆ°HierarchicalMoEMLPBlock")
            return False


if __name__ == "__main__":
    print("=" * 70)
    print("å›¾åƒç¼–ç å™¨MoEå®ç°ä¸“é¡¹æµ‹è¯•")
    print("=" * 70)
    
    # æµ‹è¯•å¯¼å…¥
    import_ok = test_import_in_context()
    
    # æµ‹è¯•HierarchicalBlockåˆ›å»º
    block_ok = test_hierarchical_block_creation()
    
    # æµ‹è¯•å®Œæ•´æ¨¡å‹çš„å›¾åƒç¼–ç å™¨
    model_ok = test_image_encoder_moe()
    
    print("\n" + "=" * 70)
    print("æœ€ç»ˆç»“æœ:")
    print("=" * 70)
    
    if import_ok and block_ok and model_ok:
        print("ğŸ‰ å›¾åƒç¼–ç å™¨æ­£ç¡®ä½¿ç”¨äº†é«˜æ•ˆMoEå®ç°ï¼")
        print("âœ“ å¯¼å…¥æˆåŠŸ")
        print("âœ“ HierarchicalBlockåˆ›å»ºæ­£ç¡®")
        print("âœ“ æ¨¡å‹ä¸­ä½¿ç”¨äº†OptimizedHierarchicalMoEMLPBlock")
    elif import_ok and block_ok and not model_ok:
        print("âš  å¯¼å…¥å’Œåˆ›å»ºéƒ½æ­£å¸¸ï¼Œä½†æ¨¡å‹ä¸­æœªä½¿ç”¨ï¼š")
        print("âœ“ å¯¼å…¥æˆåŠŸ")
        print("âœ“ HierarchicalBlockåˆ›å»ºæ­£ç¡®")
        print("âœ— æ¨¡å‹å¯èƒ½æœ‰é…ç½®é—®é¢˜")
    elif import_ok and not block_ok:
        print("âš  å¯¼å…¥æˆåŠŸä½†åˆ›å»ºå¤±è´¥ï¼š")
        print("âœ“ å¯¼å…¥æˆåŠŸ")
        print("âœ— HierarchicalBlockåˆ›å»ºå¤±è´¥")
        print("âœ— å¯èƒ½æœ‰å‚æ•°ä¸å…¼å®¹é—®é¢˜")
    else:
        print("âŒ å­˜åœ¨æ ¹æœ¬æ€§é—®é¢˜ï¼š")
        print(f"  å¯¼å…¥: {'âœ“' if import_ok else 'âœ—'}")
        print(f"  åˆ›å»º: {'âœ“' if block_ok else 'âœ—'}")
        print(f"  æ¨¡å‹: {'âœ“' if model_ok else 'âœ—'}")
    
    print("=" * 70)