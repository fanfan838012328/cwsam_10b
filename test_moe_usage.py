"""
éªŒè¯æ¨¡å‹æ˜¯å¦ä½¿ç”¨äº†é«˜æ•ˆMoEå®ç°
"""

import torch
import yaml
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/mnt/fanfq/project/code/cwsam_10b')

import models


def test_moe_implementation():
    """æµ‹è¯•æ¨¡å‹å®é™…ä½¿ç”¨çš„MoEå®ç°"""
    print("éªŒè¯æ¨¡å‹ä½¿ç”¨çš„MoEå®ç°...")
    
    # 1. åŠ è½½é…ç½®
    config_path = '/mnt/fanfq/project/code/cwsam_10b/configs/XinTong/XinTong_sam_vit_h_moe_3b.yaml'
    with open(config_path, 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    
    print(f"æ¨¡å‹é…ç½®: {config['model']['name']}")
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = models.make(config['model'])
    
    # 3. æ£€æŸ¥MoEå±‚å®ç°
    print("\næ£€æŸ¥MoEå±‚å®ç°:")
    moe_layers = []
    hierarchical_layers = []
    
    for name, module in model.named_modules():
        # æ£€æŸ¥æ˜¯å¦æœ‰MoEå±‚
        if hasattr(module, 'mlp') and hasattr(module.mlp, 'experts'):
            moe_layers.append((name, module))
            
            # æ£€æŸ¥å…·ä½“çš„MoEç±»å‹
            mlp_type = type(module.mlp).__name__
            mlp_module = type(module.mlp).__module__
            
            print(f"  {name}:")
            print(f"    MLPç±»å‹: {mlp_type}")
            print(f"    æ¨¡å—è·¯å¾„: {mlp_module}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é«˜æ•ˆMoEçš„ç‰¹å¾æ–¹æ³•
            efficient_features = []
            if hasattr(module.mlp, 'load_1_5b_expert_weights'):
                efficient_features.append('load_1_5b_expert_weights')
            if hasattr(module.mlp, '_freeze_original_experts'):
                efficient_features.append('_freeze_original_experts')
            if hasattr(module.mlp, 'expert_choice_routing'):
                efficient_features.append('expert_choice_routing')
            if hasattr(module.mlp, 'use_sharding'):
                efficient_features.append('use_sharding')
            
            if efficient_features:
                print(f"    é«˜æ•ˆMoEç‰¹å¾: {efficient_features}")
            else:
                print(f"    é«˜æ•ˆMoEç‰¹å¾: æ— ")
                
        # æ£€æŸ¥HierarchicalBlock
        if 'HierarchicalBlock' in str(type(module)):
            hierarchical_layers.append((name, module))
    
    print(f"\næ€»å…±æ‰¾åˆ° {len(moe_layers)} ä¸ªMoEå±‚")
    print(f"æ€»å…±æ‰¾åˆ° {len(hierarchical_layers)} ä¸ªHierarchicalBlockå±‚")
    
    # 4. è¯¦ç»†åˆ†æç¬¬ä¸€ä¸ªMoEå±‚
    if moe_layers:
        print(f"\nè¯¦ç»†åˆ†æç¬¬ä¸€ä¸ªMoEå±‚:")
        name, module = moe_layers[0]
        print(f"  å±‚åç§°: {name}")
        print(f"  MLPç±»å‹: {type(module.mlp)}")
        print(f"  MLPæ¨¡å—: {type(module.mlp).__module__}")
        
        # æ£€æŸ¥ä¸“å®¶æ•°é‡
        if hasattr(module.mlp, 'num_experts'):
            print(f"  ä¸“å®¶æ•°é‡: {module.mlp.num_experts}")
        if hasattr(module.mlp, 'experts') and hasattr(module.mlp.experts, '__len__'):
            print(f"  å®é™…ä¸“å®¶æ•°: {len(module.mlp.experts)}")
            
        # æ£€æŸ¥æ–¹æ³•å­˜åœ¨æ€§
        methods_to_check = [
            'load_1_5b_expert_weights',
            '_freeze_original_experts', 
            'expert_choice_routing',
            'forward'
        ]
        
        print(f"  æ–¹æ³•æ£€æŸ¥:")
        for method in methods_to_check:
            has_method = hasattr(module.mlp, method)
            print(f"    {method}: {'âœ“' if has_method else 'âœ—'}")
    
    # 5. æ£€æŸ¥importæ˜¯å¦æˆåŠŸ
    print(f"\næ£€æŸ¥æ¨¡å—å¯¼å…¥çŠ¶æ€:")
    try:
        from models.mmseg.models.sam.efficient_moe import OptimizedHierarchicalMoEMLPBlock
        print("  âœ“ OptimizedHierarchicalMoEMLPBlock å¯¼å…¥æˆåŠŸ")
        print(f"    æ¨¡å—è·¯å¾„: {OptimizedHierarchicalMoEMLPBlock.__module__}")
    except ImportError as e:
        print(f"  âœ— OptimizedHierarchicalMoEMLPBlock å¯¼å…¥å¤±è´¥: {e}")
    
    try:
        from models.mmseg.models.sam.common import HierarchicalMoEMLPBlock
        print("  âœ“ HierarchicalMoEMLPBlock å¯¼å…¥æˆåŠŸ")
        print(f"    æ¨¡å—è·¯å¾„: {HierarchicalMoEMLPBlock.__module__}")
    except ImportError as e:
        print(f"  âœ— HierarchicalMoEMLPBlock å¯¼å…¥å¤±è´¥: {e}")
    
    # 6. åˆ¤æ–­ç»“æœ
    using_efficient = False
    if moe_layers:
        first_moe = moe_layers[0][1]
        if hasattr(first_moe.mlp, 'load_1_5b_expert_weights'):
            using_efficient = True
    
    print(f"\nç»“æœåˆ¤æ–­:")
    if using_efficient:
        print("  âœ“ æ¨¡å‹ä½¿ç”¨äº†é«˜æ•ˆMoEå®ç°")
        print("  âœ“ åŒ…å«æƒé‡åŠ è½½å’Œå†»ç»“åŠŸèƒ½")
    else:
        print("  âœ— æ¨¡å‹ä½¿ç”¨äº†åŸå§‹MoEå®ç°")
        print("  âœ— å¯èƒ½å›é€€åˆ°äº†HierarchicalMoEMLPBlock")
    
    return using_efficient


def test_import_directly():
    """ç›´æ¥æµ‹è¯•importèƒ½å¦æˆåŠŸ"""
    print("\n" + "="*50)
    print("ç›´æ¥æµ‹è¯•æ¨¡å—å¯¼å…¥")
    print("="*50)
    
    # æµ‹è¯•ç›´æ¥å¯¼å…¥
    print("1. æµ‹è¯•ç›´æ¥å¯¼å…¥OptimizedHierarchicalMoEMLPBlock:")
    try:
        from models.mmseg.models.sam.efficient_moe import OptimizedHierarchicalMoEMLPBlock
        print("   âœ“ å¯¼å…¥æˆåŠŸ")
        
        # å°è¯•åˆ›å»ºå®ä¾‹
        try:
            test_moe = OptimizedHierarchicalMoEMLPBlock(
                embedding_dim=128,
                mlp_dim=512,
                num_expert_groups=2,
                experts_per_group=4,
            )
            print("   âœ“ å®ä¾‹åŒ–æˆåŠŸ")
            print(f"   âœ“ ç±»å‹: {type(test_moe)}")
            return True
        except Exception as e:
            print(f"   âœ— å®ä¾‹åŒ–å¤±è´¥: {e}")
            return False
            
    except ImportError as e:
        print(f"   âœ— å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_hierarchical_block_import():
    """æµ‹è¯•HierarchicalBlockä¸­çš„å¯¼å…¥"""
    print("\n2. æµ‹è¯•HierarchicalBlockä¸­çš„importè·¯å¾„:")
    
    # æ¨¡æ‹ŸHierarchicalBlockä¸­çš„å¯¼å…¥é€»è¾‘
    try:
        from models.mmseg.models.sam.efficient_moe import OptimizedHierarchicalMoEMLPBlock
        print("   âœ“ efficient_moe.OptimizedHierarchicalMoEMLPBlock å¯¼å…¥æˆåŠŸ")
        efficient_available = True
    except ImportError as e:
        print(f"   âœ— efficient_moe.OptimizedHierarchicalMoEMLPBlock å¯¼å…¥å¤±è´¥: {e}")
        efficient_available = False
    
    try:
        from models.mmseg.models.sam.common import HierarchicalMoEMLPBlock
        print("   âœ“ common.HierarchicalMoEMLPBlock å¯¼å…¥æˆåŠŸ")
        common_available = True
    except ImportError as e:
        print(f"   âœ— common.HierarchicalMoEMLPBlock å¯¼å…¥å¤±è´¥: {e}")
        common_available = False
    
    print(f"\n   å¯¼å…¥ç»“æœ:")
    print(f"     é«˜æ•ˆMoEå¯ç”¨: {efficient_available}")
    print(f"     åŸå§‹MoEå¯ç”¨: {common_available}")
    
    if efficient_available:
        print("   âœ“ åº”è¯¥ä½¿ç”¨OptimizedHierarchicalMoEMLPBlock")
    elif common_available:
        print("   âš  å›é€€åˆ°HierarchicalMoEMLPBlock")
    else:
        print("   âœ— ä¸¤ä¸ªå®ç°éƒ½ä¸å¯ç”¨")
    
    return efficient_available


if __name__ == "__main__":
    print("=" * 60)
    print("MoEå®ç°éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•ç›´æ¥å¯¼å…¥
    import_ok = test_import_directly()
    
    # æµ‹è¯•HierarchicalBlockå¯¼å…¥è·¯å¾„
    hierarchical_import_ok = test_hierarchical_block_import()
    
    # æµ‹è¯•æ¨¡å‹ä¸­çš„å®é™…ä½¿ç”¨
    model_using_efficient = test_moe_implementation()
    
    print("\n" + "=" * 60)
    print("æœ€ç»ˆç»“æœ:")
    print("=" * 60)
    
    if import_ok and hierarchical_import_ok and model_using_efficient:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("âœ“ é«˜æ•ˆMoEæ¨¡å—å¯ä»¥æ­£å¸¸å¯¼å…¥")
        print("âœ“ HierarchicalBlockå¯¼å…¥è·¯å¾„æ­£ç¡®")
        print("âœ“ æ¨¡å‹å®é™…ä½¿ç”¨äº†é«˜æ•ˆMoEå®ç°")
    elif import_ok and hierarchical_import_ok and not model_using_efficient:
        print("âš  éƒ¨åˆ†é—®é¢˜:")
        print("âœ“ é«˜æ•ˆMoEæ¨¡å—å¯ä»¥æ­£å¸¸å¯¼å…¥")
        print("âœ“ HierarchicalBlockå¯¼å…¥è·¯å¾„æ­£ç¡®")
        print("âœ— ä½†æ¨¡å‹å¯èƒ½æ²¡æœ‰ä½¿ç”¨é«˜æ•ˆMoEå®ç°")
        print("éœ€è¦æ£€æŸ¥HierarchicalBlockçš„åˆ›å»ºé€»è¾‘")
    else:
        print("âŒ å­˜åœ¨é—®é¢˜:")
        print(f"  æ¨¡å—å¯¼å…¥: {'âœ“' if import_ok else 'âœ—'}")
        print(f"  å¯¼å…¥è·¯å¾„: {'âœ“' if hierarchical_import_ok else 'âœ—'}")
        print(f"  æ¨¡å‹ä½¿ç”¨: {'âœ“' if model_using_efficient else 'âœ—'}")
    
    print("=" * 60)