"""
éªŒè¯å‚æ•°å†»ç»“æ˜¯å¦æ­£ç¡®
"""

import torch
import yaml
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/mnt/fanfq/project/code/cwsam_10b')

import models


def test_parameter_freezing():
    """æµ‹è¯•å‚æ•°å†»ç»“åŠŸèƒ½"""
    print("æµ‹è¯•å‚æ•°å†»ç»“åŠŸèƒ½...")
    
    # 1. åŠ è½½é…ç½®
    config_path = '/mnt/fanfq/project/code/cwsam_10b/configs/XinTong/XinTong_sam_vit_h_moe_3b.yaml'
    with open(config_path, 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = models.make(config['model'])
    
    # 3. ç»Ÿè®¡åˆå§‹å‚æ•°çŠ¶æ€
    total_params_before = sum(p.numel() for p in model.parameters())
    trainable_params_before = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params_before = total_params_before - trainable_params_before
    
    print(f"\næƒé‡åŠ è½½å‰:")
    print(f"  æ€»å‚æ•°: {total_params_before:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params_before:,}")
    print(f"  å†»ç»“å‚æ•°: {frozen_params_before:,}")
    
    # 4. åŠ è½½checkpointå¹¶åˆå§‹åŒ–MoE
    checkpoint_path = config.get('sam_checkpoint')
    if os.path.exists(checkpoint_path):
        print(f"\nåŠ è½½checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # åº”ç”¨åŸºç¡€æƒé‡
        model_dict = model.state_dict()
        filtered_dict = {k: v for k, v in checkpoint.items() 
                        if k in model_dict and model_dict[k].shape == v.shape}
        model.load_state_dict(filtered_dict, strict=False)
        
        # åˆå§‹åŒ–é«˜æ•ˆMoEæƒé‡ï¼ˆè¿™é‡Œä¼šå†»ç»“å‚æ•°ï¼‰
        print("\nåˆå§‹åŒ–é«˜æ•ˆMoEæƒé‡...")
        moe_layers = []
        for name, module in model.named_modules():
            if hasattr(module, 'load_1_5b_expert_weights'):
                print(f"  æ­£åœ¨åˆå§‹åŒ–: {name}")
                module.load_1_5b_expert_weights(checkpoint)
                moe_layers.append((name, module))
        
        print(f"æ€»å…±åˆå§‹åŒ–äº† {len(moe_layers)} ä¸ªMoEå±‚")
    else:
        print(f"è­¦å‘Š: checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return False
    
    # 5. ç»Ÿè®¡æƒé‡åŠ è½½åçš„å‚æ•°çŠ¶æ€
    total_params_after = sum(p.numel() for p in model.parameters())
    trainable_params_after = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params_after = total_params_after - trainable_params_after
    
    print(f"\næƒé‡åŠ è½½å:")
    print(f"  æ€»å‚æ•°: {total_params_after:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params_after:,}")
    print(f"  å†»ç»“å‚æ•°: {frozen_params_after:,}")
    
    # 6. åˆ†æå‚æ•°å˜åŒ–
    frozen_change = frozen_params_after - frozen_params_before
    trainable_change = trainable_params_after - trainable_params_before
    
    print(f"\nå‚æ•°å˜åŒ–:")
    print(f"  æ–°å¢å†»ç»“å‚æ•°: {frozen_change:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°å˜åŒ–: {trainable_change:,}")
    print(f"  å†»ç»“æ¯”ä¾‹: {frozen_params_after/total_params_after*100:.1f}%")
    print(f"  å¯è®­ç»ƒæ¯”ä¾‹: {trainable_params_after/total_params_after*100:.1f}%")
    
    # 7. éªŒè¯å†»ç»“æ˜¯å¦æ­£ç¡®
    if frozen_change > 0:
        print(f"\nâœ“ å‚æ•°å†»ç»“æˆåŠŸï¼å†»ç»“äº† {frozen_change:,} ä¸ªå‚æ•°")
        
        # è¯¦ç»†åˆ†ææ¯ä¸ªMoEå±‚çš„å†»ç»“æƒ…å†µ
        print("\nè¯¦ç»†å†»ç»“æƒ…å†µ:")
        for layer_name, moe_module in moe_layers[:3]:  # åªæ˜¾ç¤ºå‰3å±‚
            layer_total = 0
            layer_frozen = 0
            layer_trainable = 0
            
            for param in moe_module.parameters():
                layer_total += param.numel()
                if param.requires_grad:
                    layer_trainable += param.numel()
                else:
                    layer_frozen += param.numel()
            
            print(f"  {layer_name}:")
            print(f"    æ€»å‚æ•°: {layer_total:,}")
            print(f"    å†»ç»“: {layer_frozen:,} ({layer_frozen/layer_total*100:.1f}%)")
            print(f"    å¯è®­ç»ƒ: {layer_trainable:,} ({layer_trainable/layer_total*100:.1f}%)")
        
        return True
    else:
        print(f"\nâœ— å‚æ•°å†»ç»“å¤±è´¥ï¼æ²¡æœ‰å‚æ•°è¢«å†»ç»“")
        return False


def test_specific_expert_freezing():
    """æµ‹è¯•ç‰¹å®šä¸“å®¶çš„å†»ç»“"""
    print("\n" + "="*50)
    print("æµ‹è¯•ç‰¹å®šä¸“å®¶å†»ç»“åŠŸèƒ½")
    print("="*50)
    
    from models.mmseg.models.sam.efficient_moe import EfficientMoEMLPBlock
    
    # åˆ›å»ºæµ‹è¯•ç”¨çš„ä¸“å®¶æƒé‡
    fake_checkpoint = {}
    for expert_idx in range(16):
        base_key = f'image_encoder.blocks.28.mlp.experts.{expert_idx}'
        fake_checkpoint[f'{base_key}.0.weight'] = torch.randn(5120, 1280)
        fake_checkpoint[f'{base_key}.0.bias'] = torch.randn(5120)
        fake_checkpoint[f'{base_key}.2.weight'] = torch.randn(1280, 5120)
        fake_checkpoint[f'{base_key}.2.bias'] = torch.randn(1280)
    
    # æµ‹è¯•EfficientMoEMLPBlock
    print("\næµ‹è¯•EfficientMoEMLPBlockå†»ç»“...")
    moe_block = EfficientMoEMLPBlock(
        embedding_dim=1280,
        mlp_dim=5120,
        num_experts=32,  # 32ä¸ªä¸“å®¶ï¼Œå‰16ä¸ªåº”è¯¥è¢«å†»ç»“
        use_expert_choice=True,
        use_sharding=False,
    )
    
    # åŠ è½½å‰ç»Ÿè®¡
    total_before = sum(p.numel() for p in moe_block.parameters())
    trainable_before = sum(p.numel() for p in moe_block.parameters() if p.requires_grad)
    
    print(f"  åŠ è½½å‰: æ€»å‚æ•°={total_before:,}, å¯è®­ç»ƒ={trainable_before:,}")
    
    # åŠ è½½æƒé‡ï¼ˆä¼šè‡ªåŠ¨å†»ç»“å‰16ä¸ªä¸“å®¶ï¼‰
    moe_block.load_1_5b_expert_weights(fake_checkpoint)
    
    # åŠ è½½åç»Ÿè®¡
    total_after = sum(p.numel() for p in moe_block.parameters())
    trainable_after = sum(p.numel() for p in moe_block.parameters() if p.requires_grad)
    frozen_after = total_after - trainable_after
    
    print(f"  åŠ è½½å: æ€»å‚æ•°={total_after:,}, å¯è®­ç»ƒ={trainable_after:,}, å†»ç»“={frozen_after:,}")
    
    # éªŒè¯å‰16ä¸ªä¸“å®¶æ˜¯å¦è¢«å†»ç»“
    frozen_experts = 0
    trainable_experts = 0
    
    for expert_id in range(moe_block.num_experts):
        expert = moe_block.experts[expert_id]
        expert_frozen = all(not p.requires_grad for p in expert.parameters())
        
        if expert_frozen:
            frozen_experts += 1
        else:
            trainable_experts += 1
    
    print(f"  ä¸“å®¶çŠ¶æ€: å†»ç»“ä¸“å®¶={frozen_experts}, å¯è®­ç»ƒä¸“å®¶={trainable_experts}")
    
    if frozen_experts == 16 and trainable_experts == 16:
        print("  âœ“ ä¸“å®¶å†»ç»“æ­£ç¡®ï¼å‰16ä¸ªä¸“å®¶è¢«å†»ç»“ï¼Œå16ä¸ªå¯è®­ç»ƒ")
        return True
    else:
        print("  âœ— ä¸“å®¶å†»ç»“é”™è¯¯ï¼")
        return False


if __name__ == "__main__":
    print("=" * 60)
    print("å‚æ•°å†»ç»“éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•å®Œæ•´æ¨¡å‹
    success1 = test_parameter_freezing()
    
    # æµ‹è¯•ç‰¹å®šæ¨¡å—
    success2 = test_specific_expert_freezing()
    
    print("\n" + "=" * 60)
    if success1 and success2:
        print("ğŸ‰ æ‰€æœ‰å†»ç»“æµ‹è¯•é€šè¿‡ï¼")
        print("âœ“ 1.5Bæƒé‡æ­£ç¡®åŠ è½½å¹¶å†»ç»“")
        print("âœ“ æ–°ä¸“å®¶å‚æ•°å¯ä»¥æ­£å¸¸è®­ç»ƒ")
        print("âœ“ å‚æ•°å†»ç»“ç­–ç•¥å·¥ä½œæ­£å¸¸")
    else:
        print("âŒ éƒ¨åˆ†å†»ç»“æµ‹è¯•å¤±è´¥")
    print("=" * 60)